{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1rhxCcqptXD7z7yjruSvEn2DGivsXkENt","timestamp":1687098857523}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"FrjVPBoG4tSl"},"source":["\n","\n","# Hyperparameter tuning (HPT)\n","\n","* Hyper-parameters are parameters that are not directly learnt within estimators.\n","* In `sklearn`, they are passed as arguments to the constructor of the estimator classes. e.g. regularization rate `alpha` for Lasso.\n","\n","> It is possible and recommended to search the hyper-parameter space for the best cross validation score."]},{"cell_type":"markdown","metadata":{"id":"k3qK4vAk62q2"},"source":["Any parameter provided when constructing an estimator may be optimized in this manner.\n","\n","> Specifically, to find the names and current values for all parameters for a given estimator, use:\n","  ```\n","  estimator.get_param()\n","  ```\n","\n","Note that it is common that a small subset of those parameters can have a large impact on the predictive or computation performance of the model while others can be left to their default values.\n","> It is recommended to read the docstring of the estimator class to get a finer understanding of their expected behavior, possibly by reading the enclosed reference to the literature."]},{"cell_type":"markdown","metadata":{"id":"xo7qIg257IOI"},"source":["## Components of hyperparameter search\n","\n","Hyper parameter search consists of\n","* an estimator (regressor or classifier);\n","* a parameter space;\n","* a method for searching or sampling candidates;\n","* a cross-validation scheme; and\n","* a score function.\n"]},{"cell_type":"markdown","metadata":{"id":"Hlg1rECz7iRE"},"source":["## HPT approaches\n","\n","Two generic HPT approaches are:\n","* `GridSearchCV` exhaustively considers all parameter combinations for given values.\n","* `RandomizedSearchCV` can sample a given number of candidates from a parameter space with a specified distribution.\n","\n","There are successive halving counterparts `HalvingGridSearchCV` and `HalvingRandomSearchCV`, that can be much faster at finding a good\n","parameter combination.\n","\n","A few models allow for specialized, efficient search strategy for HPT and we would use those alternatives whenever available."]},{"cell_type":"markdown","metadata":{"id":"vatVWwiR4QSo"},"source":["## Exhaustive grid search\n","\n","The grid search provided by `GridSearchCV` exhaustively generates candidates from a grid of parameter values specified with the param_grid parameter. For instance, the following param_grid:"]},{"cell_type":"code","metadata":{"id":"yLz0ZAe26CO7"},"source":["param_grid = [\n","  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n","  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n"," ]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KKIafIor-U_Y"},"source":["specifies that two grids should be explored:\n","* One with a linear kernel and `C` values in [1, 10, 100, 1000], and\n","* The second one with an RBF kernel, and the cross-product of `C` values ranging in [1, 10, 100, 1000] and `gamma` values in [0.001, 0.0001]."]},{"cell_type":"markdown","metadata":{"id":"kwG3Xgx_-jFJ"},"source":["The GridSearchCV instance implements the usual estimator API:\n","> It evaluates all possible combination of parameter values when “fitting” on a dataset and retains the best combination."]},{"cell_type":"markdown","metadata":{"id":"RIleoPwc4VLJ"},"source":["## Randomized parameter optimization\n","\n","* Implements a randomized search over parameters, where each setting is sampled from a distribution over possible parameter values.\n","\n","* This has two main benefits over an exhaustive search:\n","  * A budget can be chosen independent of the number of parameters and possible values.\n","  * Adding parameters that do not influence the performance does not decrease efficiency.\n","\n","* A dictionary is used to specify how parameters should be sampled.\n","\n","* Additionally, a computation budget in terms of the number of sampled candidates or sampling iterations, is specified using the `n_iter` parameter.\n"]},{"cell_type":"markdown","metadata":{"id":"TiQUAAjg_o9I"},"source":["For each parameter, either a distribution over possible values or a list of discrete choices (which will be sampled uniformly) can be specified:\n","\n","```\n","{'C': scipy.stats.expon(scale=100), 'gamma': scipy.stats.expon(scale=.1),\n","  'kernel': ['rbf'], 'class_weight':['balanced', None]}\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"kEl57pCN4bKK"},"source":["## Tips for parameter search\n","\n","* When evaluating the resulting model, it is important to do it on held-out samples that were not seen during the grid search process: it is recommended to split the data into a *development set* (to be fed to the `GridSearchCV` instance) and an *evaluation set* to compute performance metrics.\n","  > This can be done by using the `train_test_split` utility function.\n","* Specify an objective metric: single or multiple through `scoring` parameter.\n","* `GridSearchCV` and `RandomizedSearchCV` allow searching over parameters of composite or nested estimators such as `Pipeline`.\n","* [Parallelism] The parameter search tools evaluate each parameter combination on each data fold independently.\n","  > Computations can be run in parallel by using the keyword n_jobs=-1\n","* [Robustness for failure]\n","  * Some parameter settings may result in a failure to fit one or more folds of the data.\n","  * By default, this will cause the entire search to fail, even if some parameter settings could be fully evaluated.\n","  * Setting `error_score=0` (or `=np.NaN`) will make the procedure robust to such failure, issuing a warning and setting the score for that fold to `0` (or `NaN`), but completing the search."]},{"cell_type":"markdown","metadata":{"id":"iesmMh_xCgnR"},"source":["*Note to slidemaker*: Include an example for Pipeline and gridsearch for polynomial regression."]},{"cell_type":"markdown","metadata":{"id":"6pEBg-CW4gFp"},"source":["## Alternatives to brute force parameter search"]},{"cell_type":"markdown","metadata":{"id":"qhHlvI45C0ga"},"source":["### Model-specific CV\n","\n","* Some models can fit data for a range of values of some parameter almost as efficiently as fitting the estimator for a single value of the parameter.\n","\n","* This feature can be leveraged to perform a more efficient cross-validation used for model selection of this parameter.\n","\n","> The most common parameter amenable to this strategy is the parameter encoding the strength of the regularizer. Here we compute the regularization path of the estimator."]},{"cell_type":"markdown","metadata":{"id":"csYBfQjUDf7m"},"source":["List of models for regression:\n","\n","* `linear_model.LassoCV`\n","* `linear_model.LassoLarsCV`\n","* `linear_model.RidgeCV`\n","* `linear_model.ElasticNetCV`"]}]}