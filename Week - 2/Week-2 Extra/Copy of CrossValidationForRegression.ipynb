{"cells":[{"cell_type":"markdown","metadata":{"id":"zOx5laNOXUWz"},"source":["# Model evaluation\n","\n","* Scoring\n","* Tools"]},{"cell_type":"markdown","metadata":{"id":"mNT5IRdHo67W"},"source":["# Regression metrics\n","\n","* The `sklearn.metrics` module implements several loss, score, and utility functions to measure regression performance.\n","* A lot of those have been enhanced to handle the `multioutput` case. e.g .`mean_squared_error`, `mean_absolute_error`, `explained_variance_score` and  `r2_score`"]},{"cell_type":"markdown","metadata":{"id":"7ytsWasNq0Yy"},"source":["## Multioutput scoring\n","\n","Needs to specify the way the scores or losses for each individual target should be averaged.\n","  * The default is `uniform_average`, which specifies a uniformly weighted mean over outputs.\n","  * If an ndarray of shape (`n_outputs,`) is passed, then its entries are interpreted as weights and an according weighted average is returned.\n","  * If multioutput is `raw_values` is specified, then all unaltered individual scores or losses will be returned in an array of shape (`n_outputs,`).\n","\n","The `r2_score` and `explained_variance_score` accept an additional value `variance_weighted` for the `multioutput` parameter.\n","* This option leads to a weighting of each individual score by the variance of the corresponding target variable.\n","* If the target variables are of different scale, then this score puts more importance on well explaining the higher variance variables."]},{"cell_type":"markdown","metadata":{"id":"glcfbGr7qZB3"},"source":["Let's study these metrics one by one."]},{"cell_type":"markdown","metadata":{"id":"WJiteb2_r7To"},"source":["## Explained variance score\n","\n","The [`explained_variance_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.explained_variance_score.html#sklearn.metrics.explained_variance_score) computes the explained variance regression score.\n","\n","If $\\hat{\\mathbf{y}}$ is the estimated target output, $\\mathbf{y}$ the corresponding (correct) target output, and `Var` is Variance, the square of the standard deviation, then the explained variance is estimated as follow:\n","\n","\\begin{equation}\n","  \\mathrm{explained\\_var} = 1 - \\frac{\\mathrm{Var}\\left(\\mathbf{y}-\\hat{\\mathbf{y}}\\right)}{\\mathrm{Var}(\\mathbf{y})}\n","\\end{equation}\n","\n","The best possible score is 1.0, lower values are worse."]},{"cell_type":"markdown","metadata":{"id":"KfgaUHQEtnQF"},"source":["**Example with single output regression**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1288,"status":"ok","timestamp":1663670180078,"user":{"displayName":"Debajyoti Biswas","userId":"02568079466891335003"},"user_tz":-330},"id":"E2ZAvkuMtmqd","outputId":"d6436518-111e-4bfc-c0e9-4d942458dad1"},"outputs":[{"data":{"text/plain":["0.9571734475374732"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.metrics import explained_variance_score\n","y_true = [3, -0.5, 2, 7]\n","y_pred = [2.5, 0.0, 2, 8]\n","explained_variance_score(y_true, y_pred)"]},{"cell_type":"markdown","metadata":{"id":"Z5RBpaYQBZy5"},"source":[]},{"cell_type":"markdown","metadata":{"id":"EzZfxspWtsi9"},"source":["**Example with multioutput regression**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":414,"status":"ok","timestamp":1634951732656,"user":{"displayName":"Ashish Tendulkar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13977487600466438153"},"user_tz":-330},"id":"mbd1OSnutxG9","outputId":"3fb439b0-98df-4102-c9dc-cd24039bc5c7"},"outputs":[{"data":{"text/plain":["array([0.96774194, 1.        ])"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["y_true = [[0.5, 1], [-1, 1], [7, -6]]\n","y_pred = [[0, 2], [-1, 2], [8, -5]]\n","explained_variance_score(y_true, y_pred, multioutput='raw_values')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9YH0EsdyFZog"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"q1KihVUluD53"},"source":["Weighted average by passing the weights in `multioutput` parameter."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":394,"status":"ok","timestamp":1634951741172,"user":{"displayName":"Ashish Tendulkar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13977487600466438153"},"user_tz":-330},"id":"Li9TfTvbt6OI","outputId":"cf6773b5-8a66-4797-dff8-06957dddd104"},"outputs":[{"data":{"text/plain":["0.9903225806451612"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["explained_variance_score(y_true, y_pred, multioutput=[0.3, 0.7])"]},{"cell_type":"markdown","metadata":{"id":"-EYaL1SdvqHA"},"source":["## Max-error\n","\n","The [`max_error`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.max_error.html#sklearn.metrics.max_error) function computes the maximum residual error, a metric that captures the worst case error between the predicted value and the true value.\n","\n","In a perfectly fitted single output regression model, max_error would be 0 on the training set and though this would be highly unlikely in the real world, this metric shows the extent of error that the model had when it was fitted.\n","\n","If $\\hat{y}^{(i)}$ is the predicted value of the $i$-th sample, and\n"," $y^{(i)}$ is the corresponding true value, then the max error is defined as\n","\n","$$\n","  \\mathrm{Max\\ error}(\\mathbf{y}, \\hat{\\mathbf{y}}) = \\mathrm{max}\\left(|y^{(i)} - \\hat{y}^{(i)}|\\right)\n","$$\n","\n","Here is a small example of usage of the max_error function:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":364,"status":"ok","timestamp":1634952558293,"user":{"displayName":"Ashish Tendulkar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13977487600466438153"},"user_tz":-330},"id":"sIavect0xBGM","outputId":"5967c77e-6615-431f-d63a-928459031cd2"},"outputs":[{"data":{"text/plain":["6"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.metrics import max_error\n","y_true = [3, 2, 7, 1]\n","y_pred = [9, 2, 7, 1]\n","max_error(y_true, y_pred)"]},{"cell_type":"markdown","metadata":{"id":"LnrRELUIw8eX"},"source":["The `max_error` does not support multioutput."]},{"cell_type":"markdown","metadata":{"id":"EIOGNqL-xNhY"},"source":["## Mean absolute error (MAE)\n","\n","The [`mean_absolute_error`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html#sklearn.metrics.mean_absolute_error) function computes mean absolute error, a risk metric corresponding to the expected value of the absolute error loss or $l_1$-norm loss.\n","\n","$$\n","   \\mathrm{MAE}(\\mathbf{y}, \\mathbf{\\hat{y}}) = \\frac{1}{n} \\sum_{i=1}^{n} |\\mathbf{y}^{(i)} - \\hat{\\mathbf{y}}^{(i)} |\n","$$"]},{"cell_type":"markdown","metadata":{"id":"rPla9Qb6xlsE"},"source":["**Single output**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1663784362463,"user":{"displayName":"VIKAS KUMAR BANCHHOR","userId":"16599693805519877553"},"user_tz":-330},"id":"uWvcGGfWxheL","outputId":"fd78ea62-fffe-444f-8b1d-a56ba8d08ee1"},"outputs":[{"data":{"text/plain":["0.5"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.metrics import mean_absolute_error\n","y_true = [3, -0.5, 2, 7]\n","y_pred = [2.5, 0.0, 2, 8]\n","mean_absolute_error(y_true, y_pred)"]},{"cell_type":"markdown","metadata":{"id":"Jx0msclLxpn9"},"source":["**Multi-output**: `Uniform average` (Default)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1663784362464,"user":{"displayName":"VIKAS KUMAR BANCHHOR","userId":"16599693805519877553"},"user_tz":-330},"id":"pn9pPmRYxvZ7","outputId":"fcd7ccfd-6ebb-4029-f30d-68ec0526c84b"},"outputs":[{"data":{"text/plain":["0.75"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["y_true = [[0.5, 1], [-1, 1], [7, -6]]\n","y_pred = [[0, 2], [-1, 2], [8, -5]]\n","mean_absolute_error(y_true, y_pred)"]},{"cell_type":"markdown","metadata":{"id":"-aj8VVK5yaRx"},"source":["**Multi-output: raw**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1663784362464,"user":{"displayName":"VIKAS KUMAR BANCHHOR","userId":"16599693805519877553"},"user_tz":-330},"id":"7BxsgMD3ygkH","outputId":"68d1e4ee-6691-4481-b468-fbe74d2f41f4"},"outputs":[{"data":{"text/plain":["array([0.5, 1. ])"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["mean_absolute_error(y_true, y_pred, multioutput='raw_values')"]},{"cell_type":"markdown","metadata":{"id":"pWK1GobKxsj8"},"source":["**Multi-output weighted**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1663784362464,"user":{"displayName":"VIKAS KUMAR BANCHHOR","userId":"16599693805519877553"},"user_tz":-330},"id":"VDgQ0tbLxxCO","outputId":"3a644044-198d-46b3-b952-a38ea423338a"},"outputs":[{"data":{"text/plain":["0.85"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["mean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])"]},{"cell_type":"markdown","metadata":{"id":"THCvGWgK0T7R"},"source":["## Mean squared error (MSE)\n","\n","The [`mean_squared_error`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error) function computes mean square error, a risk metric corresponding to the expected value of the squared (quadratic) error or loss.\n","\n","$$\n","   \\mathrm{MSE}(\\mathbf{y}, \\mathbf{\\hat{y}}) = \\frac{1}{n} \\sum_{i=1}^{n} \\left(\\mathbf{y}^{(i)} - \\hat{\\mathbf{y}}^{(i)} \\right)^2\n","$$"]},{"cell_type":"markdown","metadata":{"id":"eZgT01PL1pA-"},"source":["**Single output**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":420,"status":"ok","timestamp":1634953976602,"user":{"displayName":"Ashish Tendulkar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13977487600466438153"},"user_tz":-330},"id":"sxv4clCd1n26","outputId":"80f4ee8f-4e57-4c65-ea80-106f43523d8b"},"outputs":[{"data":{"text/plain":["0.375"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.metrics import mean_squared_error\n","y_true = [3, -0.5, 2, 7]\n","y_pred = [2.5, 0.0, 2, 8]\n","mean_squared_error(y_true, y_pred)"]},{"cell_type":"markdown","metadata":{"id":"pEQf8iBy16nn"},"source":["**Multioutput**: `uniform_average` (default)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":652,"status":"ok","timestamp":1634953977856,"user":{"displayName":"Ashish Tendulkar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13977487600466438153"},"user_tz":-330},"id":"V9ELJdiy15VO","outputId":"ede9a29d-d003-4bfe-a549-ba2497f6cdbe"},"outputs":[{"data":{"text/plain":["0.7083333333333334"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["y_true = [[0.5, 1],[-1, 1],[7, -6]]\n","y_pred = [[0, 2],[-1, 2],[8, -5]]\n","mean_squared_error(y_true, y_pred)\n"]},{"cell_type":"markdown","metadata":{"id":"kjMXQtUi2DH-"},"source":["**Multioutput**: raw output"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":563,"status":"ok","timestamp":1634953984624,"user":{"displayName":"Ashish Tendulkar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13977487600466438153"},"user_tz":-330},"id":"qS9W6XrL2Dr3","outputId":"72fc0d66-da4c-48a8-b1f0-b45e04e22369"},"outputs":[{"data":{"text/plain":["array([0.41666667, 1.        ])"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["mean_squared_error(y_true, y_pred, multioutput='raw_values')"]},{"cell_type":"markdown","metadata":{"id":"KlBk80iJ2EQm"},"source":["**Multioutput**: weighted"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":370,"status":"ok","timestamp":1634953986946,"user":{"displayName":"Ashish Tendulkar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13977487600466438153"},"user_tz":-330},"id":"1jl0kS452FcL","outputId":"a2825178-dd35-4d27-b1b6-9e1626a9c96e"},"outputs":[{"data":{"text/plain":["0.825"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["mean_squared_error(y_true, y_pred, multioutput=[0.3, 0.7])"]},{"cell_type":"markdown","metadata":{"id":"YrXoFvnT1t4G"},"source":["## Root mean squared error (RMSE)\n","\n","By setting `squared` argument in `mean_squared_error` metric to `False`, we obtain root mean squared error."]},{"cell_type":"markdown","metadata":{"id":"00KCxnJq2yMf"},"source":["Single output RMSE:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":392,"status":"ok","timestamp":1634954108406,"user":{"displayName":"Ashish Tendulkar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13977487600466438153"},"user_tz":-330},"id":"TsCzlLUB1s39","outputId":"8cbc65de-9145-4dce-994a-97fc6f67dcbc"},"outputs":[{"data":{"text/plain":["0.6123724356957945"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["\n","y_true = [3, -0.5, 2, 7]\n","y_pred = [2.5, 0.0, 2, 8]\n","mean_squared_error(y_true, y_pred, squared=False)"]},{"cell_type":"markdown","metadata":{"id":"JD9kH0eD2022"},"source":["Multioutput RMSE with default `uniform_average`:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":371,"status":"ok","timestamp":1634954110870,"user":{"displayName":"Ashish Tendulkar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13977487600466438153"},"user_tz":-330},"id":"CrsZLf3S1zlO","outputId":"e5ee9d88-55bd-48f1-e3f6-1362eacfebfc"},"outputs":[{"data":{"text/plain":["0.8416254115301732"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["y_true = [[0.5, 1],[-1, 1],[7, -6]]\n","y_pred = [[0, 2],[-1, 2],[8, -5]]\n","\n","mean_squared_error(y_true, y_pred, squared=False)"]},{"cell_type":"markdown","metadata":{"id":"CQu8NrbD3JBl"},"source":["Multioutput: Raw RMSE values"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":393,"status":"ok","timestamp":1634954134317,"user":{"displayName":"Ashish Tendulkar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13977487600466438153"},"user_tz":-330},"id":"UZAYyKfI3B3n","outputId":"a6857bdd-1960-497f-8233-e05a9528f2c2"},"outputs":[{"data":{"text/plain":["array([0.41666667, 1.        ])"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["mean_squared_error(y_true, y_pred, squared=False, multioutput='raw_values')"]},{"cell_type":"markdown","metadata":{"id":"pNg-vQm-3OIN"},"source":["Multioutput: Weighted RMSE values"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":377,"status":"ok","timestamp":1634954152869,"user":{"displayName":"Ashish Tendulkar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13977487600466438153"},"user_tz":-330},"id":"4sU7xLJb3GrL","outputId":"de54587b-1718-485b-8950-f69c068eb28b"},"outputs":[{"data":{"text/plain":["0.9082951062292475"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["mean_squared_error(y_true, y_pred, squared=False, multioutput=[0.3, 0.7])"]},{"cell_type":"markdown","metadata":{"id":"Cnno7qYi5LDn"},"source":["## R² score (coefficient of determination)\n","The [`r2_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score) function computes the coefficient of determination, usually denoted as R².\n","* It represents the proportion of variance (of $\\mathbf{y}$) that has been explained by the independent variables in the model.\n","\n","* It provides an indication of goodness of fit and therefore a measure of how well unseen samples are likely to be predicted by the model, through the proportion of explained variance.\n","\n","* As such variance is dataset dependent, R² may not be meaningfully comparable across different datasets.\n","\n","* Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse).\n","\n","* A constant model that always predicts the expected value of y, disregarding the input features, would get a R² score of 0.0.\n","\n","It is computed as\n","$$\n","   R²(\\mathbf{y}, \\mathbf{\\hat{y}}) = 1 - \\frac{\\sum_{i=1}^{n} \\left(\\mathbf{y}^{(i)} - \\hat{\\mathbf{y}}^{(i)} \\right)^2}{\\sum_{i=1}^{n} \\left(\\mathbf{y}^{(i)} - \\bar{\\mathbf{y}}^{(i)} \\right)^2}\n","$$\n","\n","where, $\\bar{y} = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbf{y}^{(i)}$\n","\n","Note that r2_score calculates unadjusted R² without correcting for bias in sample variance of y."]},{"cell_type":"markdown","metadata":{"id":"QsSjY5Xc69pB"},"source":["Example 1: Single output"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1611,"status":"ok","timestamp":1651464019959,"user":{"displayName":"Debajyoti Biswas ee14d302","userId":"16910797007013702735"},"user_tz":-330},"id":"PVTKxW2I60ZK","outputId":"302ae123-cbac-4ad7-e0bc-dace04d5d5f6"},"outputs":[{"data":{"text/plain":["0.9486081370449679"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.metrics import r2_score\n","y_true = [3, -0.5, 2, 7]\n","y_pred = [2.5, 0.0, 2, 8]\n","r2_score(y_true, y_pred)"]},{"cell_type":"markdown","metadata":{"id":"Jomt7YCt7BHx"},"source":["Example 2: Multi output - `variance_weighted` leads to a weighting of each individual score by the variance of the corresponding target variable."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GcFcG9SV606y"},"outputs":[],"source":["y_true = [[0.5, 1], [-1, 1], [7, -6]]\n","y_pred = [[0, 2], [-1, 2], [8, -5]]\n","r2_score(y_true, y_pred, multioutput='variance_weighted')"]},{"cell_type":"markdown","metadata":{"id":"GXf5Lskh7LcO"},"source":["Example 3: Multi output - `uniform_average`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"awabdA7p67Kv"},"outputs":[],"source":["y_true = [[0.5, 1], [-1, 1], [7, -6]]\n","y_pred = [[0, 2], [-1, 2], [8, -5]]\n","r2_score(y_true, y_pred, multioutput='uniform_average')"]},{"cell_type":"markdown","metadata":{"id":"Ghsg9OPV7czP"},"source":["Example 4: Multi output - `raw_values`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bh2FgF1768K2"},"outputs":[],"source":["r2_score(y_true, y_pred, multioutput='raw_values')"]},{"cell_type":"markdown","metadata":{"id":"mTPtBIau7jSN"},"source":["Example 5: Multi output - weighted"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k_SqtZim67l7"},"outputs":[],"source":["r2_score(y_true, y_pred, multioutput=[0.3, 0.7])"]},{"cell_type":"markdown","metadata":{"id":"kE1Y9yuY3Y00"},"source":["There are a few other regression metrics that are included in `sklearn`.  We encourage you to check them out:\n","* [`mean_squared_logarithmic_error`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_log_error.html#sklearn.metrics.mean_squared_log_error) is best used when targets have exponential growth, such as population counts, average sales of a commodity over a span of years etc. Note that this metric penalizes an under-predicted estimate greater than an over-predicted estimate.\n","* [`mean_absolute_percentage_error`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_percentage_error.html#sklearn.metrics.mean_absolute_percentage_error) is sensitive to relative error.\n","* [`median_absolute_error`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.median_absolute_error.html#sklearn.metrics.median_absolute_error) is robust to outliers. It does not support multioutput case.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8OVJHZttXeLZ"},"source":["# Model quality scoring"]},{"cell_type":"markdown","metadata":{"id":"6tatBLUkXUUs"},"source":["`sklearn` provides three different APIs for evaluating the quality of model prediction:\n","* **`score` method** of estimators with default evaluation criteria.\n","* **Scoring parameter** with model-evaluation tools using cv e.g. `model_selection.cross_val_score`.\n","* **Metric functions** implemented in `sklearn.metrics`.\n","\n","`sklearn` also provides **Dummy estimators** for creating a baseline model."]},{"cell_type":"markdown","metadata":{"id":"xN7Hya18vVsf"},"source":["# Scoring parameter\n","\n","Model selection and evaluation methods like `model_selection.GridSearchCV` and `model_selection.cross_val_score` take a `scoring` parameter that specifies metric for evaluating the estimator.\n"]},{"cell_type":"markdown","metadata":{"id":"ij9zjiVhYzLl"},"source":["## Predefined values\n","\n","* Uses scorer object.\n","* Scorer object convention: *Higher return values are better than the lower values.*\n","* The error metrics are available as negated version of the metric. For example, `metrics.mean_squared_error` is available as `neg_mean_squared_error`.\n"]},{"cell_type":"markdown","metadata":{"id":"MQmorzB1Z7gs"},"source":["| `scoring` | Function |\n","| --------- | -------- |\n","| `explained_variance` | [`metrics.explained_variance_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.explained_variance_score.html#sklearn.metrics.explained_variance_score) |\n","| `max_error` | [`metrics.max_error`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.max_error.html#sklearn.metrics.max_error)|\n","| `neg_mean_absolute_error` | [`metrics.mean_absolute_error`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html#sklearn.metrics.mean_absolute_error) |\n","| `neg_mean_squared_error` | [`metrics.mean_squared_error`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error) |\n","| `neg_root_mean_squared_error` | [`metrics.mean_squared_error`]()|\n","| `neg_mean_squared_log_error` | [`metrics.mean_squared_log_error`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_log_error.html#sklearn.metrics.mean_squared_log_error)|\n","| `neg_median_absolute_error` | [`metrics.median_absolute_error`]() |\n","| `r2` | [`metrics.r2_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.median_absolute_error.html#sklearn.metrics.median_absolute_error) |"]},{"cell_type":"markdown","metadata":{"id":"PhhuYUu1efWv"},"source":["## Scoring strategy from metric functions\n","\n","`sklearn.metrics` exposes a set of simple functions measuring a prediction error given ground truth and prediction:\n","* functions ending with `_score` follows convention of higher the better.\n","* functions ending with `_error` follows convention of lower the better.\n","  * While converting into a scorer object using `make_scorer`, set `greater_is_better` parameter to `False`.\n","\n","Many metrics are given names to be used as `scoring` value.\n","* Generate appropriate scoring objects using [`make_scorer`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html#sklearn.metrics.make_scorer)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vwp_cyxhsOP3"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"LoJYto2Tlnyn"},"source":["#### Note on `make_scorer`\n","\n","* Makes a scorer from a performance metric or loss function.\n","* It's a factory function that wraps scoring functions for use in model evaluation and selection.\n","* It takes a score function, such as `accuracy_score`, `mean_squared_error` and returns a callable that scores an estimator’s output.\n","* The signature of the call is (`estimator, X, y`) where\n","  * `estimator` is the model to be evaluated,\n","  * `X` is the data and `y` is the ground truth labeling (or `None` in the case of unsupervised models).\n"]},{"cell_type":"markdown","metadata":{"id":"0cZhdgsTgHql"},"source":["### Scorer from existing metrics\n","\n","One typical use case is to wrap an existing metric functions with non-default value:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O8ltC-TrXRPk"},"outputs":[],"source":["from sklearn.metrics import fbeta_score, make_scorer\n","\n","# `make_scorer` for metric function fbeta_score with\n","# non-default value for `beta`\n","ftwo_scorer = make_scorer(fbeta_score, beta=2)\n","\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.svm import LinearSVC\n","\n","# using callable object returned by `make_scorer` in scoring\n","# parameter.\n","grid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]},\n","                    scoring=ftwo_scorer, cv=5)"]},{"cell_type":"markdown","metadata":{"id":"XVXKH7bphZ8y"},"source":["### Custom scorer\n","\n","`sklearn` provides a way to build a completely custom scorer object use `make_scorer`, that can take several parameters:\n","* Python function for custom scoring\n","* whether the python function returns a score (`greater_is_better=True`, the default) or a loss (`greater_is_better=False`).\n","  * In case of a loss, the output of the python function is negated by the scorer object, conforming to the cross validation convention that scorers return higher values for better models.\n","\n","* *for classification metrics only*: whether the python function requires continuous decision certainties (`needs_threshold=True`). The default value is `False`.\n","\n","* Any additional parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":361,"status":"ok","timestamp":1634949094776,"user":{"displayName":"Ashish Tendulkar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13977487600466438153"},"user_tz":-330},"id":"eG1x2R1vi25Z","outputId":"be797262-beb1-4cf3-cbcf-049b0df40b74"},"outputs":[{"name":"stdout","output_type":"stream","text":["Scoring from custom function: 1\n","Scoring from callable: -1\n"]}],"source":["import numpy as np\n","from sklearn.metrics import make_scorer\n","\n","# Python function for custom scoring.\n","def my_custom_loss_func(y_true, y_pred):\n","    diff = np.abs(y_true - y_pred).max()\n","    return diff\n","\n","# Using the custom function in `make_scorer`.\n","# score will negate the return value of my_custom_loss_func\n","# given the values for X and y defined below.\n","score = make_scorer(my_custom_loss_func, greater_is_better=False)\n","X = [[1], [1]]\n","y = [0, 1]\n","\n","\n","from sklearn.dummy import DummyClassifier\n","clf = DummyClassifier(strategy='most_frequent', random_state=0)\n","clf = clf.fit(X, y)\n","print (\"Scoring from custom function:\",\n","       my_custom_loss_func(y, clf.predict(X)))\n","\n","# Using the callable returned by make_scorer.\n","print (\"Scoring from callable:\", score(clf, X, y))\n","\n","# Note: `score` negates return value of custom loss function."]},{"cell_type":"markdown","metadata":{"id":"8kHDMThej8nh"},"source":["### Implement own scoring object\n","\n","* `sklearn` provide a way to write flexible model scorer by constructing your own scoring objects from scratch without using `make_scorer`.\n","\n","* Needs to adhere to the protocols specified by the following two rules:\n","  * It can be called with parameters (`estimator, X, y`), where\n","    * `estimator` is the model for evaluation,\n","    * `X` is validation data, and  `y` is the ground truth target for `X` (in the supervised case) or `None` (in the unsupervised case).\n","\n","* It returns a floating point number that quantifies the prediction quality of `estimator` on `X`, with reference to `y`.\n","> Again, by convention *higher numbers are better*, so if your scorer returns loss, that value should be negated.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3mFTmTq6miUz"},"source":["### Using multiple metrics for evaluation\n","\n","`sklearn` provides a way to use multiple metrics for model evaluation and selection.\n","\n","Three ways to specify multiple scoring metrics for `scoring` parameter:"]},{"cell_type":"markdown","metadata":{"id":"rMlgIqSWm7oS"},"source":["(1) As a iterable of string metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vqpmy5qIm_JI"},"outputs":[],"source":["scoring = ['accuracy', 'precision']"]},{"cell_type":"markdown","metadata":{"id":"k-sEasGwnCg0"},"source":["(2) As a `dict` mapping the scorer name to the scoring function\n","\n","> Note that the dict values can either be scorer functions or one of the predefined metric strings."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JyinTy7vnPYW"},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","from sklearn.metrics import make_scorer\n","scoring = {'accuracy': make_scorer(accuracy_score),\n","           'prec': 'precision'}"]},{"cell_type":"markdown","metadata":{"id":"I1zzfCK3nS9-"},"source":["(3) As a callable that returns a dictionary of scores"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9FAFRShonp_4"},"outputs":[],"source":["from sklearn.metrics import confusion_matrix\n","\n","def confusion_matrix_scorer(clf, X, y):\n","     y_pred = clf.predict(X)\n","     cm = confusion_matrix(y, y_pred)\n","\n","     # Returning multiple metrics\n","     return {'tn': cm[0, 0], 'fp': cm[0, 1],\n","             'fn': cm[1, 0], 'tp': cm[1, 1]}"]},{"cell_type":"markdown","metadata":{"id":"XKqpXBSI8B9R"},"source":["# Dummy regressor\n","\n","When doing supervised learning, a simple sanity check consists of comparing one’s estimator against simple rules of thumb."]},{"cell_type":"markdown","metadata":{"id":"U579SkpT8XT-"},"source":["[`DummyRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyRegressor.html#sklearn.dummy.DummyRegressor) implements four simple rules of thumb or strategies for regression:\n","* `mean` always predicts the mean of the training targets.\n","* `median` always predicts the median of the training targets.\n","* `quantile` always predicts a user provided quantile of the training targets.\n","* `constant` always predicts a constant value that is provided by the user.\n","\n","In all these strategies, the predict method completely ignores the input data.\n","\n","It is useful as a simple baseline to compare with other (real) regressors. **Do not use it for real problems.**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":460,"status":"ok","timestamp":1663670952771,"user":{"displayName":"Debajyoti Biswas","userId":"02568079466891335003"},"user_tz":-330},"id":"OwbjD8FM87Nr","outputId":"8128262a-cb71-4e94-ea8b-bb253b1d4cfe"},"outputs":[{"data":{"text/plain":["0.0"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np\n","from sklearn.dummy import DummyRegressor\n","X = np.array([1.0, 2.0, 3.0, 4.0])\n","y = np.array([2.0, 3.0, 5.0, 10.0])\n","dummy_regr = DummyRegressor(strategy=\"mean\")\n","dummy_regr.fit(X, y)\n","dummy_regr.predict(X)\n","dummy_regr.score(X, y)"]},{"cell_type":"markdown","metadata":{"id":"Me0-GYUm9JwK"},"source":[" When the accuracy of regressor is too close to dummy regressor, it probably means that something went wrong: features are not helpful, or a hyperparameter is not correctly tuned."]},{"cell_type":"markdown","metadata":{"id":"8uvO4mwdJl6S"},"source":["# CV: Model performance evaluation\n","\n","As you may recall, in k-fold cross-validation scheme, we partition the training data into $k$ partitions, train with $k-1$ of those and validate the model performance on the remaining partition.\n","\n","The performance measure reported by k-fold cross-validation is then the average of the values computed in the loop.\n","\n","This approach can be computationally expensive, but does not waste too much data (as is the case when fixing an arbitrary validation set).\n","\n","The final evaluation is performed on the test set.\n","\n","**Note to slidemaker: Add figure https://scikit-learn.org/stable/_images/grid_search_cross_validation.png in the slide deck."]},{"cell_type":"markdown","metadata":{"id":"woDSftoxLKmY"},"source":["Let's first check out `sklearn` utilities for splitting dataset according to different cross validation strategies."]},{"cell_type":"markdown","metadata":{"id":"xuolJ7MOLYvN"},"source":["In regression setup, we made an iid assumption about the data. (Independent and identically distributed).\n","\n","It means all samples are drawn from the same generative process and that the generative process is assumed to have no memory of past generated samples.\n","\n","`sklearn` provides cross validation utilities for non-iid data too.  We won't be covering it in this course.  However we are listing it down here for your reference:\n","* [Time series aware cross validation](https://scikit-learn.org/stable/modules/cross_validation.html#timeseries-cv) for samples generated using a time dependent process.\n","* [Group-wise cross validation](https://scikit-learn.org/stable/modules/cross_validation.html#group-cv) for samples generated from a process with a group structure (samples collected from different subjects, experiments, measurement devices).\n","\n","There are cv utilities for classification that will be covered in the classification module.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qiVB1GhUNNPU"},"source":["### k-fold\n","\n","* [`KFold`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold) divides all the samples in  groups of samples, called folds (if $k=n$, this is equivalent to the Leave One Out strategy), of equal sizes (if possible).\n","\n","* The prediction function is learned using $k-1$ folds, and the fold left out is used for test."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1663671352981,"user":{"displayName":"Debajyoti Biswas","userId":"02568079466891335003"},"user_tz":-330},"id":"MNMOx2uzNuLd","outputId":"c1f74226-0564-444c-b7e3-b501c9218e86"},"outputs":[{"name":"stdout","output_type":"stream","text":["Split #1, Train fold: [2 3] | Test fold: [0 1]\n","Split #2, Train fold: [0 1] | Test fold: [2 3]\n"]}],"source":["# This is am example of 2 fold cv on a dataset with 4 samples.\n","import numpy as np\n","from sklearn.model_selection import KFold\n","\n","X = [\"a\", \"b\", \"c\", \"d\"]\n","kf = KFold(n_splits=2)\n","\n","split = 1\n","for train, test in kf.split(X):\n","    print(\"Split #%d, Train fold: %s | Test fold: %s\" % (split, train, test))\n","    split += 1"]},{"cell_type":"markdown","metadata":{"id":"pU-ty-TDOGau"},"source":["The training and test set can be constructed with `train` and `test` indices."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":379,"status":"ok","timestamp":1634960384346,"user":{"displayName":"Ashish Tendulkar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13977487600466438153"},"user_tz":-330},"id":"fFTvUGbpOPVg","outputId":"4f676832-da79-4410-9b31-420e9e8d081e"},"outputs":[{"name":"stdout","output_type":"stream","text":["START OF FOLD\n","X_train: [[-1. -1.]\n"," [ 2.  2.]]\n","y_train: [0 1]\n","X_test: [[0. 0.]\n"," [1. 1.]]\n","X_test: [[0. 0.]\n"," [1. 1.]]\n","END OF FOLD\n","\n","START OF FOLD\n","X_train: [[0. 0.]\n"," [1. 1.]]\n","y_train: [0 1]\n","X_test: [[-1. -1.]\n"," [ 2.  2.]]\n","X_test: [[-1. -1.]\n"," [ 2.  2.]]\n","END OF FOLD\n","\n"]}],"source":["X = np.array([[0., 0.], [1., 1.], [-1., -1.], [2., 2.]])\n","y = np.array([0, 1, 0, 1])\n","\n","kf = KFold(n_splits=2)\n","for train, test in kf.split(X):\n","  X_train, X_test, y_train, y_test = X[train], X[test], y[train], y[test]\n","  print ('START OF FOLD')\n","  print ('X_train: %s'% X_train)\n","  print ('y_train: %s'% y_train)\n","  print ('X_test: %s'% X_test)\n","  print ('X_test: %s'% X_test)\n","  print ('END OF FOLD\\n')"]},{"cell_type":"markdown","metadata":{"id":"Mj7fmr6EPsJe"},"source":["## Repeated k-fold\n","\n","[`RepeatedKFold`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedKFold.html#sklearn.model_selection.RepeatedKFold) repeats K-Fold `n_repeats` times.\n","\n","It can be used when one requires to run KFold `n_repeats` times, producing different splits in each repetition."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":311,"status":"ok","timestamp":1639717220250,"user":{"displayName":"Debajyoti Biswas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgJcdYLR50sLMbR0NI9-GNNobiSmysRIvDA6ZE0=s64","userId":"18377199666023252143"},"user_tz":-330},"id":"LpJIUYDpQSva","outputId":"d3487b2c-c371-4655-9681-49a00bfc6ddd"},"outputs":[{"name":"stdout","output_type":"stream","text":["split #1, [0 2] [1 3]\n","split #2, [1 3] [0 2]\n","split #3, [0 2] [1 3]\n","split #4, [1 3] [0 2]\n"]}],"source":["# Example of 2-fold K-Fold repeated 2 times:\n","import numpy as np\n","from sklearn.model_selection import RepeatedKFold\n","\n","X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n","random_state = 42\n","\n","rkf = RepeatedKFold(n_splits=2, n_repeats=2, random_state=random_state)\n","\n","split = 1\n","for train, test in rkf.split(X):\n","    print(\"split #%d, %s %s\" % (split, train, test))\n","    split += 1"]},{"cell_type":"markdown","metadata":{"id":"WZ4H07cDQpvw"},"source":["## Leave one out (LOO)\n","\n","[LeaveOneOut](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneOut.html#sklearn.model_selection.LeaveOneOut) (or LOO) is a simple cross-validation.\n","\n","Each learning set is created by taking all the samples except one, the test set being the sample left out. Thus, for  samples, we have  different training sets and  different tests set.\n","\n","This cross-validation procedure does not waste much data as only one sample is removed from the training set:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":368,"status":"ok","timestamp":1634962212442,"user":{"displayName":"Ashish Tendulkar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13977487600466438153"},"user_tz":-330},"id":"_eva389URzVV","outputId":"12f487f5-6587-4095-9f68-6dff50780288"},"outputs":[{"name":"stdout","output_type":"stream","text":["split# 1, [1 2 3] [0]\n","split# 2, [0 2 3] [1]\n","split# 3, [0 1 3] [2]\n","split# 4, [0 1 2] [3]\n"]}],"source":["from sklearn.model_selection import LeaveOneOut\n","\n","X = [1, 2, 3, 4]\n","loo = LeaveOneOut()\n","\n","split = 1\n","for train, test in loo.split(X):\n","    print(\"split# %d, %s %s\" % (split, train, test))\n","    split += 1"]},{"cell_type":"markdown","metadata":{"id":"rNaIn79ER9S4"},"source":["There are a few caveats that we should be aware of:\n","\n","When compared with $k$-fold cross validation,\n","* In terms of the number of models:\n","  * We builds $n$ models from  $n$ samples instead of $k$ models, where $n > k$.\n","  * Moreover, each is trained on $(n-1)$ samples rather than $(k-1)n/k$. In both ways, assuming $k$ is not too large and $k < n$, LOO is more computationally expensive than k-fold cross validation.\n","\n","* In terms of accuracy,\n","  * LOO often results in high variance as an estimator for the test error.\n","  * Intuitively, since $n-1$ of the $n$ samples are used to build each model, models constructed from folds are virtually identical to each other and to the model built from the entire training set.\n","\n","> As a general rule, most authors, and empirical evidence, suggest that 5- or 10- fold cross validation should be preferred to LOO."]},{"cell_type":"markdown","metadata":{"id":"21JrHscLQu5h"},"source":["## Leave P out (LPO)\n","\n","[`LeavePOut`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeavePOut.html#sklearn.model_selection.LeavePOut) is very similar to `LeaveOneOut` as it creates all the possible training/test sets by removing $p$ samples from the complete set.\n","* For $n$ samples, this produces $n \\choose p$ train-test pairs.\n","* Unlike `LeaveOneOut` and `KFold`, the test sets will overlap for $p > 1$.\n","\n","Example of Leave-2-Out on a dataset with 4 samples:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":501,"status":"ok","timestamp":1634962154432,"user":{"displayName":"Ashish Tendulkar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13977487600466438153"},"user_tz":-330},"id":"dvMTkn56Uet3","outputId":"69b72195-7639-4f2c-d780-4c5fd709c00b"},"outputs":[{"name":"stdout","output_type":"stream","text":["split #1, [2 3] [0 1]\n","split #2, [1 3] [0 2]\n","split #3, [1 2] [0 3]\n","split #4, [0 3] [1 2]\n","split #5, [0 2] [1 3]\n","split #6, [0 1] [2 3]\n"]}],"source":["from sklearn.model_selection import LeavePOut\n","\n","X = np.ones(4)\n","lpo = LeavePOut(p=2)\n","\n","split = 1\n","for train, test in lpo.split(X):\n","    print(\"split #%d, %s %s\" % (split, train, test))\n","    split += 1"]},{"cell_type":"markdown","metadata":{"id":"BJ2eLfN0Q1rU"},"source":["## Random permutation (shuffle and split)\n","\n","The [`ShuffleSplit`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit)  iterator will generate a user defined number of independent train/test dataset splits.\n","* Samples are first shuffled and then split into a pair of train and test sets.\n","* It is possible to control the randomness for reproducibility of the results by explicitly seeding the random_state pseudo random number generator.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":435,"status":"ok","timestamp":1634962122989,"user":{"displayName":"Ashish Tendulkar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13977487600466438153"},"user_tz":-330},"id":"dT06d4V2U-lN","outputId":"4bd6e26a-48ea-4cfe-e9aa-acbfe86648f1"},"outputs":[{"name":"stdout","output_type":"stream","text":["split #1, [9 1 6 7 3 0 5] [2 8 4]\n","split #2, [2 9 8 0 6 7 4] [3 5 1]\n","split #3, [4 5 1 0 6 9 7] [2 3 8]\n","split #4, [2 7 5 8 0 3 4] [6 1 9]\n","split #5, [4 1 0 6 8 9 3] [5 2 7]\n"]}],"source":["from sklearn.model_selection import ShuffleSplit\n","X = np.arange(10)\n","ss = ShuffleSplit(n_splits=5, test_size=0.25, random_state=0)\n","\n","split = 1\n","for train_index, test_index in ss.split(X):\n","    print(\"split #%d, %s %s\" % (split, train_index, test_index))\n","    split += 1"]},{"cell_type":"markdown","metadata":{"id":"XOBx8b1DU-_C"},"source":["`ShuffleSplit` is a good alternative to `KFold` cross validation that allows a finer control on the number of iterations and the proportion of samples on each side of the train / test split."]},{"cell_type":"markdown","metadata":{"id":"XGH7GAIeKocN"},"source":["## CV metrics"]},{"cell_type":"markdown","metadata":{"id":"XLaw-CX8rr2R"},"source":["### Scoring by CV: `cross_val_score`\n","\n","[`cross_val_score`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score) is the simplest way to evaluate a score by cv.\n","\n","It has the following important arguments:\n","1. `estimator` object implementing `fit`.\n","2. `X`: feature matrix of shape (n, m)\n","3. `y`: label vector of shape (n, ) or (n, k) for multi-output\n","4. `scoring` parameter to specify how score should be calculated. If `None` the default scorer of estimator is used.\n","5. `cv`\n","  * `None`: uses 5-fold cv.\n","  * int, to specify # of folds in `KFold`.\n","  * CV splitter\n","  * Iterable yielding (train, test) splits of array indices\n","6. `error_score` is the value to assign to the score if an error occurs in fitting.\n","  * `raise` - error is raised\n","  * numeric - `FitFailedWarning` is used\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":966,"status":"ok","timestamp":1635035159924,"user":{"displayName":"Ashish Tendulkar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13977487600466438153"},"user_tz":-330},"id":"yLOEq7jTr4fw","outputId":"7264d163-34d3-4e41-8e76-5e111d6a1257"},"outputs":[{"name":"stdout","output_type":"stream","text":["[0.33150734 0.08022311 0.03531764]\n"]}],"source":["from sklearn import datasets, linear_model\n","from sklearn.model_selection import cross_val_score\n","\n","diabetes = datasets.load_diabetes()\n","X = diabetes.data[:150]\n","y = diabetes.target[:150]\n","lasso = linear_model.Lasso()\n","\n","# Note the arguments:\n","# 1. estimator: lasso\n","# 2. feature matrix: X\n","# 3. label vector: y\n","# 4. # of cv folds: cv (By defaukt uses `Kfold` cv)\n","score = cross_val_score(lasso, X, y, cv=3)\n","\n","# Since the model is fitted with cv = 3, we would get\n","# a score vector with shape (3, )\n","print(score)"]},{"cell_type":"markdown","metadata":{"id":"5lubQ2CotCRa"},"source":["We can obtain mean and standard deviation of score as follows:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":430,"status":"ok","timestamp":1635035464765,"user":{"displayName":"Ashish Tendulkar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13977487600466438153"},"user_tz":-330},"id":"O2n4uyRhtHtW","outputId":"0f86f576-37e6-4900-d2f1-548f9165d0db"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mean of score:0.15\n","Standard deviation of score:0.13\n"]}],"source":["print (\"Mean of score:%4.2f\"%score.mean())\n","print (\"Standard deviation of score:%4.2f\"%score.std())"]},{"cell_type":"markdown","metadata":{"id":"8MTU-mGEtaTo"},"source":["We can use other cv strategy, by passing the cv iterator."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":405,"status":"ok","timestamp":1635035558059,"user":{"displayName":"Ashish Tendulkar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13977487600466438153"},"user_tz":-330},"id":"RWbiORBmtipP","outputId":"c2ccda3b-17df-4824-a2a8-1ce5a42bb8eb"},"outputs":[{"data":{"text/plain":["array([0.30553672, 0.22064577, 0.17962466])"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.model_selection import ShuffleSplit\n","n_samples = X.shape[0]\n","cv = ShuffleSplit(n_splits=3, test_size=0.3, random_state=0)\n","cross_val_score(lasso, X, y, cv=cv)"]},{"cell_type":"markdown","metadata":{"id":"PbNQwQN7wcf7"},"source":["Using iterable for `cv`:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":438,"status":"ok","timestamp":1635036406206,"user":{"displayName":"Ashish Tendulkar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13977487600466438153"},"user_tz":-330},"id":"kw4H4p97wnV8","outputId":"4dd1c96b-ff2d-44a1-8f73-e8ee70aca415"},"outputs":[{"data":{"text/plain":["array([0.29173079, 0.33493918])"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np\n","\n","def custom_cv_2folds(X):\n","  '''Creates two folds from the input.'''\n","  n = X.shape[0]\n","  i = 1\n","  while i <= 2:\n","      idx = np.arange(n * (i - 1) / 2, n * i / 2, dtype=int)\n","      yield idx, idx\n","      i += 1\n","\n","custom_cv = custom_cv_2folds(X)\n","cross_val_score(lasso, X, y, cv=custom_cv)"]},{"cell_type":"markdown","metadata":{"id":"QeCTKcXYxJv7"},"source":["### Multiple metric evaluation: `cross_validate`\n","\n","* Many parameters listed in `cross_val_score` are applicable here too.\n","\n","* It however differs in two ways with `cross_val_score`:\n","  * Allows multiple metric specification\n","  * Returns a dict containing fit-times, scores-times in addition to test scores.\n","\n","* The multiple metrics can be specified either as a list, tuple or set of predefined scorer names, Or as a dict mapping scorer name to a predefined or custom scoring function.\n","\n","* Return dict for *single metric evaluation* contains the following keys:\n","['test_score', 'fit_time', 'score_time']\n","\n","* And for multiple metric evaluation, the return value is a dict with the following keys - ['test_<scorer1_name>', 'test_<scorer2_name>', 'test_<scorer...>', 'fit_time', 'score_time']\n","\n","* `return_train_score` is used to specify if we desire to evaluate the scores on training set.\n","  * `True` evaluates score on training set.\n","  * `False` [default] does not evaluate on training set.  It leads to saving of computation time.\n","\n","* `return_estimator` parameters helps us to specify if we want to retain the estimator fitted on each training set in cv."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PUA9EEAt0_7M"},"outputs":[],"source":["from sklearn.model_selection import cross_validate\n","?cross_validate"]},{"cell_type":"markdown","metadata":{"id":"KoUkIIVK1LJ8"},"source":["Example of specifying multiple metrics:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":411,"status":"ok","timestamp":1635037246639,"user":{"displayName":"Ashish Tendulkar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13977487600466438153"},"user_tz":-330},"id":"gDgWW3CtzY9M","outputId":"74ecc61a-df4c-4f3e-d402-d89b018ec10a"},"outputs":[{"data":{"text/plain":["['fit_time',\n"," 'score_time',\n"," 'test_neg_mean_absolute_error',\n"," 'test_neg_mean_squared_error']"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["scoring = ['neg_mean_absolute_error', 'neg_mean_squared_error']\n","scores = cross_validate(lasso, X, y, cv=3, scoring=scoring)\n","sorted(scores.keys())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":393,"status":"ok","timestamp":1635037307080,"user":{"displayName":"Ashish Tendulkar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13977487600466438153"},"user_tz":-330},"id":"8Av_oDle0Ci3","outputId":"59a98fd2-c89d-4667-b269-9045dde0bf99"},"outputs":[{"name":"stdout","output_type":"stream","text":["[-3635.51152303 -3573.34242148 -6114.78229547]\n"]}],"source":["print (scores['test_neg_mean_squared_error'])"]},{"cell_type":"markdown","metadata":{"id":"IxVa6PIL0aqZ"},"source":["Example of single metric evaluation:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":440,"status":"ok","timestamp":1635037468667,"user":{"displayName":"Ashish Tendulkar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13977487600466438153"},"user_tz":-330},"id":"ptLtIjCC0iz4","outputId":"c591caf0-194d-4c7e-e6fb-cabde8bd1b42"},"outputs":[{"data":{"text/plain":["dict_keys(['fit_time', 'score_time', 'estimator', 'test_score'])"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["scores = cross_validate(lasso, X, y, cv=3,\n","                        scoring='neg_mean_squared_error',\n","                        return_estimator=True)\n","scores.keys()"]},{"cell_type":"markdown","metadata":{"id":"uCltRkar0Bs3"},"source":["Specifying multiple metrics through dictionary"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":382,"status":"ok","timestamp":1635037827248,"user":{"displayName":"Ashish Tendulkar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13977487600466438153"},"user_tz":-330},"id":"uo2qQNwY1mdu","outputId":"9f79c75a-5846-409a-ccf0-34c4687beca4"},"outputs":[{"data":{"text/plain":["['fit_time', 'score_time', 'test_mae', 'test_mse']"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import make_scorer\n","\n","scoring = {'mae': 'neg_mean_absolute_error',\n","           'mse': make_scorer(mean_squared_error)}\n","scores = cross_validate(lasso, X, y, cv=3, scoring=scoring)\n","sorted(scores.keys())"]},{"cell_type":"markdown","metadata":{"id":"-h4YvzBf2q6k"},"source":["### Predictions by CV: `cross_val_predict`\n","\n","* The function `cross_val_predict` has a similar interface to `cross_val_score`, but returns, for each element in the input, the prediction that was obtained for that element when it was in the test set.\n","\n","* Only cross-validation strategies that assign all elements to a test set exactly once can be used (otherwise, an exception is raised)."]},{"cell_type":"markdown","metadata":{"id":"49DAQGI33FW3"},"source":["Note on inappropriate use of `cross_val_predict`\n","\n","> The result of `cross_val_predict` may be different from those obtained using `cross_val_score` as the elements are grouped in different ways.\n","\n","> The function `cross_val_score` takes an average over cross-validation folds, whereas `cross_val_predict` simply returns the labels (or probabilities) from several distinct models undistinguished.\n","\n","Hence `cross_val_predict` is not an appropriate measure of generalisation error."]},{"cell_type":"markdown","metadata":{"id":"JytkEypL9Cqf"},"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1zI6EvoIGCMBersos7-z8fYH_UlPpSGlo","timestamp":1687098950608},{"file_id":"1OohmBVkOOseUJrhgJXJG3QEvhCbJUjZR","timestamp":1635142194172}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":0}